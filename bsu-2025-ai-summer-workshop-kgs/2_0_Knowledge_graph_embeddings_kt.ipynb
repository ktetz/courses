{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBOPBvvsytzB"
   },
   "source": [
    "# Knowledge Graph Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYOcqOWmyxqg"
   },
   "source": [
    "In this notebook, you'll learn how to compute knowledge embeddings using the Hopwise library.\n",
    "\n",
    "What you‚Äôll do:\n",
    "\n",
    "* 1Ô∏è‚É£ Train TransE knowledge embedding model\n",
    "\n",
    "* 2Ô∏è‚É£ Visualize and interpret the learned embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HF72BZ2Uys85"
   },
   "source": [
    "### ‚öôÔ∏è Setup Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h7h6-PP-qPJ"
   },
   "source": [
    "1. Import the necessary module to access Google Drive from Colab and mount you Google Drive to the Colab enviroment. This allows you to access files and folders stored in your Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16898,
     "status": "ok",
     "timestamp": 1752048407793,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "plTIqQQx0FoS",
    "outputId": "92da0702-8c69-4945-85f8-72138e24bebf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADSdj9-I-qPK"
   },
   "source": [
    "2. Install the hopwise libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FV9snacRXjsq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install hopwise[cli,tsne]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qabqssKc-qPK"
   },
   "source": [
    "3. Install the openTSNE libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1752003639665,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "qJ9OhuC1dksy",
    "outputId": "b6034c71-9b2c-4690-8a4c-2e000fbc3e34"
   },
   "outputs": [],
   "source": [
    "!uv pip install openTSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdZcbXvd-qPK"
   },
   "source": [
    "4.  To view the installed libraries in the right sidebar, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vaFe7ra-qPL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ln -s /usr/local/lib/python3.11/dist-packages /content/dist-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeF1Qa_8-qPL"
   },
   "source": [
    "5. To check if you are using the GPU, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3104,
     "status": "ok",
     "timestamp": 1752003642769,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "heXSMRgmdYch",
    "outputId": "3129e260-6eb8-49f2-b41a-86ced26ff963"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device_id = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(device_id)\n",
    "    print(f\"CUDA Device ID: {device_id}\")\n",
    "    print(f\"CUDA Device Name: {device_name}\")\n",
    "else:\n",
    "    print(\"No CUDA device is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtbIKwth07Mg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### üì¶ 0 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEd3zSC20K9-"
   },
   "source": [
    "Let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qy6jXXdq0K9_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Import required components from the hopwise library\n",
    "from hopwise.quick_start import run_hopwise\n",
    "from hopwise.config import Config\n",
    "from hopwise.data import create_dataset\n",
    "from hopwise.data.dataset import KnowledgeBasedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsShKHbr1hOg"
   },
   "source": [
    "###  üìä 1 - Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuFc3Pg8XQvj"
   },
   "source": [
    "To train the TransE model, we'll use the MovieLens 100k (ml-100k) dataset, which is integrated directly into the Hopwise library.\n",
    "\n",
    "\n",
    "**üîé About MovieLens 100k**\n",
    "\n",
    "\n",
    "The MovieLens 100k dataset is a widely used benchmark for developing and evaluating recommender systems. It was collected by the GroupLens Research Project at the University of Minnesota and is recognized for its compact size and rich information.\n",
    "\n",
    "There are multiple versions of MovieLens datasets (e.g., ml-1m, ml-10m), but for the purposes of this notebook, we adopt the [ml-100k version]( https://grouplens.org/datasets/movielens/100k/).\n",
    "\n",
    "This version contains 100,000 ratings (on a 1‚Äì5 scale), contributed by 943 users on 1,682 movies, along with user demographic data.\n",
    "\n",
    "**üìä Dataset Statistics**\n",
    "\n",
    "\n",
    "| Name    | Dates   | Users | Movies | Ratings  |\n",
    "|---------|---------|--------|--------|----------|\n",
    "| ML 100K | '97‚Äì'98 | 943    | 1,682  | 100,000  |\n",
    "\n",
    "\n",
    "**üìÅ Dataset Files in Hopwise**\n",
    "\n",
    "In the Hopwise repository, the dataset is located at: [hopwise/dataset_example/ml-100k](https://github.com/tail-unica/hopwise/tree/main/hopwise/dataset_example/ml-100k)\n",
    "\n",
    "It consists of several key files:\n",
    "\n",
    "\n",
    "| **File name** | **Content**                        | **Columns**                                |\n",
    "|------------|------------------------------------|---------------------------------------------------|\n",
    "| `.inter`   | User-item interaction              | `user_id, item_id, rating, timestamp`     |\n",
    "| `.user`    | User features                       | `user_id, age, gender, occupation, zip_code`                            |\n",
    "| `.item`    | Item features                       | `item_id, movie_title, release_year, class`                               |\n",
    "| `.kg`      | Triplets in a knowledge graph      | `head_entity, relation, tail_entity`              |\n",
    "| `.link`    | Item-entity linkage data           | `item_id, entity_id`                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzcAVC0n8bgR"
   },
   "source": [
    "**‚öôÔ∏è Dataset Loading and Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3connbMpXQvj"
   },
   "source": [
    "In this section, we configure and load the **MovieLens 100k (ml-100k)**.  We also print out key statistics about the dataset, including knowledge graph details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVMjCrcxXQvk"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Define the configuration\n",
    "# --------------------------------------------\n",
    "\n",
    "# This configuration dictionary specifies:\n",
    "# - The dataset to load ('ml-100k')\n",
    "# - The KGE model to use ('TransE')\n",
    "# - Which columns to load from the interaction and item files\n",
    "config = {\n",
    "    \"dataset\": 'ml-100k',\n",
    "    \"model\": 'TransE',\n",
    "    \"load_col\": {\n",
    "        \"inter\": [\"user_id\", \"item_id\", \"rating\", \"timestamp\"],\n",
    "        \"item\": [\"item_id\", \"movie_title\"]\n",
    "    },\n",
    "}\n",
    "\n",
    "# Convert dictionary to a Config object required by hopwise\n",
    "config = Config(config_dict=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 931,
     "status": "ok",
     "timestamp": 1752048463711,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "Z8awBAKUXQvk",
    "outputId": "d33bcf25-fc90-41f8-f76b-af6fb8c383f8"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Load the dataset\n",
    "# --------------------------------------------\n",
    "\n",
    "# Create and load the dataset using the specified configuration\n",
    "dataset = create_dataset(config)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Print dataset-level statistics\n",
    "# --------------------------------------------\n",
    "\n",
    "print(f\"üìä Dataset: {dataset.dataset_name}\")\n",
    "print(f\"Number of users: {dataset.user_num}\")\n",
    "print(f\"Number of items: {dataset.item_num}\")\n",
    "print(f\"Number of interactions: {dataset.inter_num}\")\n",
    "# print(f\"Number of movie titles: {dataset.num('movie_title')}\")\n",
    "# print(f\"Sparsity: {dataset.sparsity:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1752003683980,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "acA-FcNjXQvk",
    "outputId": "61e668e3-a4b8-4fb4-f513-e9be1d01f5ba"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Knowledge Graph\n",
    "# -------------------------\n",
    "if isinstance(dataset, KnowledgeBasedDataset):\n",
    "    print(\"\\nüìò Knowledge Graph Statistics:\")\n",
    "    print(f\"Number of entities: {dataset.entity_num}\")\n",
    "    print(f\"Number of relations: {dataset.relation_num}\")\n",
    "    print(f\"Number of KG triplets: {len(dataset.kg_feat)}\")\n",
    "\n",
    "    # Display raw triplets with IDs\n",
    "    print(\"\\nüî¢ Sample KG Triplets (IDs):\")\n",
    "    kg_ids = dataset.kg_feat[['head_id', 'relation_id', 'tail_id']].head(5)\n",
    "    display(kg_ids)\n",
    "\n",
    "    # Decode and display triplets with string names\n",
    "    decoded_kg = kg_ids.copy()\n",
    "    decoded_kg['head_name'] = decoded_kg['head_id'].apply(lambda x: dataset.id2token('head_id', x))\n",
    "    decoded_kg['relation_name'] = decoded_kg['relation_id'].apply(lambda x: dataset.id2token('relation_id', x))\n",
    "    decoded_kg['tail_name'] = decoded_kg['tail_id'].apply(lambda x: dataset.id2token('tail_id', x))\n",
    "\n",
    "    print(\"\\nüßæ Sample KG Triplets (Names):\")\n",
    "    display(decoded_kg[['head_name', 'relation_name', 'tail_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX6tAbxHD1Nk"
   },
   "source": [
    "### üìö 2 - Introduction to Knowledge Graph Embeddings in Hopwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5BhUtbWD1Nk"
   },
   "source": [
    "The Hopwise library  integrates **14 state-of-the-art Knowledge Graph Embedding (KGE) methods**, enabling reasoning over structured knowledge.\n",
    "\n",
    "‚úÖ Supported KGE methods:\n",
    "- **Translational Distance Models**: `TransE`, `TransH`, `TransD`, `TransR`, `TorusE`  \n",
    "- **Semantic Matching Models**: `RESCAL`, `DistMult`, `ComplEx`, `HolE`, `Analogy`, `TuckER`  \n",
    "- **Neural Network-based Models**: `ConvE`, `ConvKB`, `RotatE`\n",
    "\n",
    "These models enable entities and relations from a knowledge graph to be embedded into continuous vector spaces, supporting tasks like **recommendation**, and  **link prediction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgJy3AzyD1Nl"
   },
   "source": [
    "\n",
    "\n",
    "The implementation and management of these **embedding models** are handled by the **model layer**.\n",
    "\n",
    "The implementation file for each model is located in the folder: üìÅ [`hopwise/model/knowledge_graph_embedding_recommender`](https://github.com/tail-unica/hopwise/tree/main/hopwise/model/knowledge_graph_embedding_recommender)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn0H7owiKcVu"
   },
   "source": [
    "\n",
    "The **configuration** of models is managed through the **config layer**, located at: üìÅ [`hopwise/config`](https://github.com/tail-unica/hopwise/tree/main/hopwise/config)\n",
    "\n",
    "The config layer supports three flexible formats for defining model behavior:\n",
    "\n",
    "1. Configuration files (`.yaml`);\n",
    "2. Command-line arguments;\n",
    "3. Python parameter dictionaries;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKePQYZKKoBu"
   },
   "source": [
    "While users can provide their own configurations, **default settings** for each model are automatically loaded from:\n",
    "\n",
    "üìÅ [`hopwise/properties/model`](https://github.com/tail-unica/hopwise/tree/main/hopwise/properties/model)\n",
    "\n",
    "These default properties ensure that each model has a working configuration out of the box, which can then be overridden or extended as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qpI8pq6ERIb"
   },
   "source": [
    "### üï∏Ô∏è 3 - Training TransE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zJOQURUD1Nl"
   },
   "source": [
    "In this section, we'll walk through training the **TransE** model.\n",
    "\n",
    "The TransE model belongs to the **Translational distance models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY79Nxrj-qPN"
   },
   "source": [
    "<div style=\"background-color:#f0f4f8; border-left: 5px solid #4a90e2; padding:15px; margin:10px 0; border-radius:8px;\">\n",
    "  <p><b>Translational distance models:</b><br><br>\n",
    "    Transform the relationship as a distance from the <b> head entity</b> to the <b> tail entity</b> and defines the <b> scoring function</b> by the distance [1].\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GgoaRe5FEaK"
   },
   "source": [
    "#### üìê Definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLp-WzKt-qPN"
   },
   "source": [
    "<div style=\"background-color:#fff1d7; border-left: 5px solid #d6a76f; padding:15px; margin:10px 0; border-radius:8px;\">\n",
    "  <p><strong>TransE model</strong><br>\n",
    "is a representative <em>translational distance model</em> that represents <strong>entities</strong> and <strong>relations</strong> as vectors in the same semantic space of dimension ‚Ñù<sup>d</sup>, where <em>d</em> is the dimensionality of the embedding space.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlBDjXwJXQvl"
   },
   "source": [
    "A Fact is represented as a triplet **(h, r, t)** where:\n",
    "- **h** = head entity\n",
    "- **r** = relation\n",
    "- **t** = tail entity\n",
    "\n",
    "The relation **r** is interpreted as a **translation vector** connecting **h** and **t**. The model is trained such that the embedding of **h + r** is close to **t**. That is:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uPxUFN1-qPN"
   },
   "source": [
    "$$ \\mathbf{h} + \\mathbf{r} \\approx \\mathbf{t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDJpSYRdXQvl"
   },
   "source": [
    "üëâ **Intuition**: the idea is that if a head entity and a relation are known, adding them should point to the correct tail entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QojGP-68XQvl"
   },
   "source": [
    "**Scoring Function**\n",
    "\n",
    "TransE performs a linear translation and uses a **scoring function** based on the distance between **h + r** and **t**. The score **f(h, r, t)** is defined as:\n",
    "\n",
    "$$ f(h, r, t) = \\| \\mathbf{h} + \\mathbf{r} - \\mathbf{t} \\| $$\n",
    "\n",
    "TransE uses the L<sub>2</sub> or L<sub>1</sub> distance between vectors to measure how plausible a triple is. The lower the distance, the more likely the triplet is to be true.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVAir8loXQvl"
   },
   "source": [
    "The model is trained to **minimize the distance** between valid triplets and **maximize** the distance for corrupted (invalid) ones, enabling the embeddings to capture the structure and semantics of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qY1RRhx5Pfco"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mallociFrancesca/XAIKGRLGM/a77f9ea5633475efe43038ef2a11e1341342e0ef/hands-on-session/transE.png\" alt=\"TransE Diagram\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISd2d4Lfm4X1"
   },
   "source": [
    "#### ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q14GfFZ7-qPQ"
   },
   "source": [
    "To train the TransE model, we first need to define the configuration parameters for both the model architecture and the training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSneoP7_-qPQ"
   },
   "source": [
    "Using Hopwise, you can configure the model in one of the following ways:\n",
    "\n",
    "- **Python Dictionary configuration**: provide the settings directly using a Python dictionary.\n",
    "- **YAML configuration**: load settings from a `.yaml` file.\n",
    "- **Default configuration**: use the predefined configuration available at: [hopwise/properties/model/PEARLM.yaml](https://github.com/tail-unica/hopwise/blob/main/hopwise/properties/model/PEARLM.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqo4Z68nm4X2"
   },
   "outputs": [],
   "source": [
    "# Define the model and dataset to use\n",
    "model = 'TransE'\n",
    "dataset = 'ml-100k'\n",
    "\n",
    "# Custom configuration dictionary\n",
    "config_dict = {\n",
    "     # 'gpu_id': 0,  # GPU device ID to use\n",
    "    'epochs': 1,  # Number of training epochs\n",
    "    'eval_step': 50,  # Evaluation frequency (every N epochs)\n",
    "    'data_path': '/content/drive/MyDrive/XAIKGRLGM/hands-on-session/data/dataset/',  # Path to the dataset\n",
    "    'checkpoint_dir':'/content/drive/MyDrive/XAIKGRLGM/hands-on-session/data/checkpoint/',  # Directory to save model checkpoints\n",
    "    'show_progress': True  # Toggle progress bar visibility during training/evaluation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNvS0hlcLdGd"
   },
   "source": [
    "#### üîÅ Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UeTqaPb-qPQ"
   },
   "source": [
    "Now, we are ready to train the model. Using the following command.\n",
    "\n",
    "‚ö†Ô∏è We have already the **precomputed transE**. So you **don't need** to run this command now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364,
     "referenced_widgets": [
      "c7e7dd88fb89458cb99e23d728d5241a",
      "74323d8a3cd7470790c8ab50d9bdf09f",
      "6a72b4391dd04d7a852737599835ff96",
      "8b30fd5fa18c431c823c6f298f499af7",
      "ce6b7eeabc0a43bfa59002364580d207",
      "11c93da9a67b415694c66fdd5680bb4d"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 19447,
     "status": "ok",
     "timestamp": 1752003740973,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "M9LefO3em4X2",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "25b2bee2-d90d-42d6-8283-1ce012ff65d9"
   },
   "outputs": [],
   "source": [
    "run_hopwise(model=model,\n",
    "            dataset=dataset,\n",
    "            config_dict=config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K_rDdgum4X3"
   },
   "source": [
    "You should receive an output like\n",
    "\n",
    "```bash\n",
    "{'best_valid_score': {<KnowledgeEvaluationType.REC: 1>: 0.1018},\n",
    " 'valid_score_bigger': True,\n",
    " 'best_valid_result': {<KnowledgeEvaluationType.REC: 1>: OrderedDict([('recall@10',\n",
    "                0.0903),\n",
    "               ('mrr@10', 0.1924),\n",
    "               ('ndcg@10', 0.1018),\n",
    "               ('hit@10', 0.4772),\n",
    "               ('precision@10', 0.0785)])},\n",
    " 'test_result': OrderedDict([('recall@10', 0.1044),\n",
    "              ('mrr@10', 0.2241),\n",
    "              ('ndcg@10', 0.1225),\n",
    "              ('hit@10', 0.4995),\n",
    "              ('precision@10', 0.0914)])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7hOyNs_POgR"
   },
   "source": [
    "üíæ **Saving Model Outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQvFHVhzPQz6"
   },
   "source": [
    "By default, the trained model checkpoint will be saved in the `hopwise/saved` directory as a `.pth` file. The filename includes the model name and a timestamp, following this format:\n",
    "\n",
    "`TransE-Month-day-year_timestamp.pth`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoeCTQTdPp8K"
   },
   "source": [
    "If you need to change the output directory, you have two options:\n",
    "\n",
    "1. **Override it dynamically** using the `checkpoint_dir` parameter in your configuration dictionary.\n",
    "2. **Modify the default path** in the global config file:  \n",
    "   üìÑ [`hopwise/properties/overall.yaml`](https://github.com/tail-unica/hopwise/blob/main/hopwise/properties/overall.yaml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5L2ptTlRR0pv"
   },
   "source": [
    "üì• **Loading a Pretrained TransE Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNT3tnHMXQvp"
   },
   "source": [
    "\n",
    "To reuse previously trained TransE embeddings, you can simply load the saved model checkpoint. This checkpoint is stored as a `.pth` file and includes:\n",
    "\n",
    "- The model's learned parameters (i.e., the embeddings)\n",
    "- Training metadata (e.g., epoch, best validation score)\n",
    "- The full configuration used during training\n",
    "\n",
    "We use `torch.load()` to deserialize the checkpoint file and inspect its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "BASE_DIR = Path('/home/kt/code/courses/bsu-2025-ai-summer-workshop-kgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qX47XQ2NSHNP"
   },
   "outputs": [],
   "source": [
    "# Path to the saved checkpoint (update with your actual filename)\n",
    "checkpoint_name = \"/content/drive/MyDrive/XAIKGRLGM/hands-on-session/data/checkpoint/TransE-Jul-08-2025_19-42-10.pth\"\n",
    "checkpoint_name = BASE_DIR / 'data/checkpoint/TransE-Jul-08-2025_19-42-10.pth'\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_name, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03yE9g4MXQvp"
   },
   "source": [
    "The structure of the checkpoint is defined within the `Trainer` class, specifically in the `_save_checkpoint()` method located in the [`hopwise/trainer/trainer.py`](https://github.com/tail-unica/hopwise/blob/main/hopwise/trainer/trainer.py) file.\n",
    "\n",
    "This method is responsible for saving the model's state during training.\n",
    "\n",
    "```bash\n",
    "state = {\n",
    "    'config': self.config, # configuration settings used for the model and training process.\n",
    "    'epoch': epoch, # current epoch number at the time of saving\n",
    "    'cur_step': self.cur_step, # current training step\n",
    "    'best_valid_score': self.best_valid_score, #  best validation score achieved so far\n",
    "    'state_dict': self.model.state_dict(), #  model's parameters\n",
    "    'optimizer': self.optimizer.state_dict(), # state of the optimizer, which includes parameters like learning rates and momentums.\n",
    "}\n",
    "```\n",
    "\n",
    "This structure ensures that all necessary information is preserved, allowing for training to be resumed easily or for the model to be evaluated later.\n",
    "\n",
    "Print the config structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1752003853256,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "j7d6EDhnXQvp",
    "outputId": "5ae931e5-bc0a-4588-e40e-afeb77208d71"
   },
   "outputs": [],
   "source": [
    "# Display top-level keys in the checkpoint dictionary\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ji6_3nAm4X4"
   },
   "source": [
    "The learned parameters are stored in the `state_dict`. Let's visualize which embeddings have been saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1752003855509,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "J0z37Lu9m4X4",
    "outputId": "120f18e2-b261-4f2a-a91a-ddae2512c091"
   },
   "outputs": [],
   "source": [
    "# Display which weights (learned embeddings) were saved in the 'state_dict'\n",
    "checkpoint[\"state_dict\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XgM3IvDXQvq"
   },
   "source": [
    "The three `.weight` entries in the `state_dict` (`user_embedding.weight`, `entity_embedding.weight`, and `relation_embedding.weight`) are the **embedding matrices** learned during training of the TransE model.\n",
    "\n",
    "Each matrix contains a dense vector representation for a specific type of node in the knowledge graph:\n",
    "\n",
    "- `user_embedding.weight`: embeddings for all users  \n",
    "- `entity_embedding.weight`: embeddings for all entities (items, tags, etc.)  \n",
    "- `relation_embedding.weight`: embeddings for all relation types (e.g., \"watched\", \"belongs_to\")\n",
    "\n",
    "These embeddings capture the **semantic and structural role** of each node or relation in the graph, based on the TransE training objective.\n",
    "\n",
    "The typical shape of each embedding matrix is: `(num_items, embedding_dim)`\n",
    "\n",
    "For example:\n",
    "- If you have 10,000 entities and the embedding size is 100, then:  \n",
    "  `entity_embedding.weight` ‚Üí shape = `(10000, 100)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-L-_SfNXQvq",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### üîç 4 - Visualizing TransE Embeddings with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiJryYbjXQvq"
   },
   "source": [
    "\n",
    "In the previous steps, we trained a **TransE model** to learn embeddings for three types of elements in a knowledge graph:\n",
    "- **Entities** (e.g., movies, genres)\n",
    "- **Users**\n",
    "- **Relations** (e.g., \"watched\", \"belongs_to\")\n",
    "\n",
    "These embeddings exist in a high-dimensional space (e.g., 100 or 200 dimensions), which makes them difficult to interpret or visualize directly.\n",
    "\n",
    "To gain insight into how these embeddings are structured, we want to **project them into a 2D space** while preserving their relative distances.\n",
    "\n",
    "This can help us identify patterns such as clustering, separation between types, or outliers.\n",
    "\n",
    "To achieve this, we use the **t-distributed Stochastic Neighbor Embedding (t-SNE)** algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hEX43jrXQvq"
   },
   "source": [
    "\n",
    "**üß† What is t-SNE?**\n",
    "\n",
    "**t-SNE** is a nonlinear dimensionality reduction technique that:\n",
    "- Converts high-dimensional similarities between data points into conditional probabilities.\n",
    "- Projects the data into a lower-dimensional space (typically 2D or 3D).\n",
    "- Optimizes the layout to preserve local structures (i.e., nearby points remain close).\n",
    "- Is particularly effective for visualizing embeddings and clustering behavior.\n",
    "\n",
    "Unlike linear methods like PCA, t-SNE focuses on maintaining neighborhood relationships, making it well-suited for exploring how different embeddings relate to one another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FkJH7B7XQvq"
   },
   "source": [
    "To perform this visualization, we use the [`openTSNE`](https://opentsne.readthedocs.io/en/stable/tsne_algorithm.html) library [2], a fast and extensible implementation of the t-SNE algorithm.\n",
    "\n",
    "We will:\n",
    "\n",
    "* 1Ô∏è‚É£ Run t-SNE separately on user, entity, and relation embeddings;\n",
    "* 2Ô∏è‚É£ Visualize each type in its own scatter plot;\n",
    "* 3Ô∏è‚É£ Finally, combine all embeddings into a single 2D plot to explore their global structure together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpkA60IiXQvq"
   },
   "outputs": [],
   "source": [
    "# === Import the required Library ===\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import plotly.express as px\n",
    "from openTSNE import TSNE\n",
    "from hopwise.utils import init_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2EufieidzBi"
   },
   "outputs": [],
   "source": [
    "# Define the path to the saved TransE checkpoint (.pth file).\n",
    "# This file contains the model's configuration and all trained parameters (weights).\n",
    "\n",
    "checkpoint_name = \"/content/drive/MyDrive/XAIKGRLGM/hands-on-session/data/checkpoint/TransE-Jul-08-2025_19-42-10.pth\"\n",
    "checkpoint_name = BASE_DIR / 'data/checkpoint/TransE-Jul-08-2025_19-42-10.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lB_9p2HXQvq"
   },
   "outputs": [],
   "source": [
    "# === Load Pretrained TransE Model and Prepare for Visualization ===\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_name, weights_only=False)\n",
    "\n",
    "\n",
    "# Extract and apply the stored configuration to ensure consistent random seed\n",
    "# and reproducibility settings for visualization (especially important for t-SNE).\n",
    "config = checkpoint[\"config\"]\n",
    "init_seed(config[\"seed\"], config[\"reproducibility\"])\n",
    "\n",
    "\n",
    "# Convert all model weights in the checkpoint from GPU tensors to CPU NumPy arrays.\n",
    "# This is necessary because visualization tools (e.g., t-SNE, Plotly) work with NumPy.\n",
    "for weight in checkpoint[\"state_dict\"].keys():\n",
    "    checkpoint[\"state_dict\"][weight] = checkpoint[\"state_dict\"][weight].to(torch.device(\"cpu\")).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRPYNwOTXQvq"
   },
   "outputs": [],
   "source": [
    "# === Define a custom  function to generate interactive 2D scatter plots using Plotly ===\n",
    "\n",
    "# Embedding points are colored by ID or type for interpretability.\n",
    "# Color gradient represents the point IDs, with darker purple indicating lower IDs and brighter yellow indicating higher IDs.\n",
    "\n",
    "def plot_fn(embeddings, desc=\"Entity\"):\n",
    "    #ids = list(range(embeddings.shape[0]))\n",
    "    fig = px.scatter(\n",
    "        x=embeddings[:, 0],\n",
    "        y=embeddings[:, 1],\n",
    "        #color=ids,\n",
    "        labels={\"x\": \"Embedding Dimension 1\", \"y\": \"Embedding Dimension 2\", \"color\": f\"{desc} ID\"},\n",
    "        title=f\"{config['model']} {desc} Embeddings\",\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        template=\"plotly_white\",\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvJxO7i5XQvq"
   },
   "source": [
    "**Setup t-SNE for Dimensionality Reduction**\n",
    "\n",
    "Initializes the t-SNE algorithm from openTSNE to reduce high-dimensional embeddings (e.g., 100D) to 2D for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZM1ocHLBXQvq"
   },
   "outputs": [],
   "source": [
    "# Initialize t-SNE for dimensionality reduction:\n",
    "# - perplexity=30: balances local/global structure, defines how many neighbors are considered (typical range is 5‚Äì50)\n",
    "# - n_jobs=8: enables parallel processing across 8 CPU threads for faster computation\n",
    "# - initialization=\"random\": starts with a random 2D layout (instead of PCA), introducing randomness\n",
    "# - metric=\"cosine\": uses cosine distance instead of Euclidean, better suited for high-dimensional embeddings\n",
    "# - random_state=config[\"seed\"]: fixes the random seed to ensure reproducible results\n",
    "# - verbose=True: prints progress updates during optimization\n",
    "\n",
    "tsne = TSNE(\n",
    "    perplexity=30,\n",
    "    n_jobs=8,\n",
    "    initialization=\"random\",\n",
    "    metric=\"cosine\",\n",
    "    random_state=config[\"seed\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyHjxed6XQvq"
   },
   "source": [
    "Now, we are ready to run t-SNE on each embeddings type (`user`, `entity`, `relation`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBDZYmNRXQvq"
   },
   "source": [
    "**Plot Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63029,
     "status": "ok",
     "timestamp": 1752003942230,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "6IbSXcucXQvq",
    "outputId": "b5ea82f4-1dc0-48ae-c589-edbed4865daf"
   },
   "outputs": [],
   "source": [
    "# Extract the pretrained user embedding weights from the checkpoint.\n",
    "user_weights = checkpoint[\"state_dict\"][\"user_embedding.weight\"]\n",
    "\n",
    "# Apply t-SNE to reduce the user embeddings from high-dimensional space to 2D.\n",
    "tsne_embeddings_users = tsne.fit(user_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vefCeYXHXQvq"
   },
   "source": [
    "The result `tsne_embeddings_users` is also a NumPy array of shape `(N, 2)`, where `N` is the number of users.\n",
    "\n",
    "The array is ready to be plotted, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2456,
     "status": "ok",
     "timestamp": 1752003947783,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "DyYpi5URXQvr",
    "outputId": "13c27f38-e06a-41a3-8b46-da1e210ffc83"
   },
   "outputs": [],
   "source": [
    "plot_fn(tsne_embeddings_users, \"User\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc5LO6vkXQvr"
   },
   "source": [
    "* The points appear to be fairly evenly distributed across the entire plane, without significant concentrations or visible clusters.\n",
    "This behavior is clearly due to the fact that, for educational purposes, we trained the model for only one epoch, so the resulting embeddings are not very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU3tSZNVXQvr"
   },
   "source": [
    "**Plot entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107443,
     "status": "ok",
     "timestamp": 1752004057986,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "Jsgo3sYOXQvr",
    "outputId": "33c4123d-caf3-4082-cb5f-b16c0546cd31"
   },
   "outputs": [],
   "source": [
    "# Extract the pretrained entity embedding weights from the checkpoint.\n",
    "entity_weights = checkpoint[\"state_dict\"][\"entity_embedding.weight\"]\n",
    "\n",
    "# Apply t-SNE to reduce the entity embeddings from high-dimensional space to 2D.\n",
    "tsne_embeddings_entities = tsne.fit(entity_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGm8wIKMXQvr"
   },
   "source": [
    "The result `tsne_embeddings_entities` is also a NumPy array of shape `(N, 2)`, where `N` is the number of entities.\n",
    "\n",
    "The array is ready to be plotted, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1529,
     "status": "ok",
     "timestamp": 1752004066364,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "JiHRXYblXQvr",
    "outputId": "1942b455-a2b3-4fe0-f513-372769113e0d"
   },
   "outputs": [],
   "source": [
    "plot_fn(tsne_embeddings_entities, \"Entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYOepHWjXQvr"
   },
   "source": [
    "* Several tight clusters can be seen, especially in the bottom left and around the periphery.\n",
    "* These likely represent semantically similar entities (e.g., movies of the same genre, or related items).\n",
    "* The central region is densely compacted, suggesting that many entities are relatively similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVZK1yUjXQvr"
   },
   "source": [
    "**Plot Relations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1752004070235,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "Sf270qa8XQvr",
    "outputId": "a17f7b1d-491f-4648-8a19-6afe8019c56b"
   },
   "outputs": [],
   "source": [
    "# Extract the pretrained relation embedding weights from the checkpoint.\n",
    "relation_weights = checkpoint[\"state_dict\"][\"relation_embedding.weight\"]\n",
    "\n",
    "# Apply t-SNE to reduce the relation embeddings from high-dimensional space to 2D.\n",
    "tsne_embeddings_relations = tsne.fit(relation_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNgJCwzJXQvr"
   },
   "source": [
    "The result `tsne_embeddings_relations` is also a NumPy array of shape `(N, 2)`, where `N` is the number of relations.\n",
    "\n",
    "The array is ready to be plotted, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1752004072290,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "AgPxnKGqXQvr",
    "outputId": "f3331b63-12e7-45fe-b81d-3d392318223f"
   },
   "outputs": [],
   "source": [
    "plot_fn(tsne_embeddings_relations, \"Relation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQo_msMmXQvr"
   },
   "source": [
    "* The plot contains only ~25 points, corresponding to the total number of relation types in the dataset.\n",
    "\n",
    "* The distance between points may suggest semantic dissimilarity (e.g., ‚Äúrated‚Äù might be far from ‚Äúbelongs_to‚Äù).\n",
    "\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFOFVv_RXQvr"
   },
   "source": [
    "We  applied t-SNE separately to each type of embedding (`users`, `entities`, and `relations`) and visualized them in individual plots.\n",
    "\n",
    "Now, we'll combine all of these embeddings into a single unified plot to explore how they are distributed relative to one another in the same 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsBKEgNEXQvr"
   },
   "source": [
    "**Combine all embeddings into one plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHIGNBnfXQvs"
   },
   "outputs": [],
   "source": [
    "def combine_embeddings(**kwargs):\n",
    "    embeddings_list = list()\n",
    "    identifiers_list = list()\n",
    "\n",
    "    for embeddings_name, embeddings in kwargs.items():\n",
    "        embeddings_list.append(embeddings)\n",
    "        identifiers_list.extend([f\"{embeddings_name} {id}\" for id in range(embeddings.shape[0])])\n",
    "        print(f\"[+] {embeddings_name}: {embeddings.shape}\")\n",
    "\n",
    "    embeddings_list = np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "    combined_df = pd.DataFrame(\n",
    "        {\n",
    "            \"x\": embeddings_list[:, 0],\n",
    "            \"y\": embeddings_list[:, 1],\n",
    "            \"type\": [id.split(\" \")[0] for id in identifiers_list],\n",
    "            \"identifier\": identifiers_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig = px.scatter(\n",
    "        combined_df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"type\",\n",
    "        hover_data=[\"identifier\"],\n",
    "        labels={\"x\": \"Embedding Dimension 1\", \"y\": \"Embedding Dimension 2\", \"type\": \"Embedding Type\"},\n",
    "        title=f\"Visualising Combined Embeddings {checkpoint_name}\",\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        template=\"plotly_white\",\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3558,
     "status": "ok",
     "timestamp": 1752004080959,
     "user": {
      "displayName": "Francesca Malloci",
      "userId": "18211733844504426583"
     },
     "user_tz": -120
    },
    "id": "4jRq3kVQXQvs",
    "outputId": "750b4607-3bb6-4216-8425-6caf20cf26bc"
   },
   "outputs": [],
   "source": [
    "# Combine user, entity, and relation t-SNE embeddings into one visualization.\n",
    "# This allows us to see how these different types of embeddings are distributed\n",
    "# in the same 2D space and whether they cluster distinctly or overlap.\n",
    "\n",
    "combine_embeddings(\n",
    "    user=tsne_embeddings_users,\n",
    "    entity=tsne_embeddings_entities,\n",
    "    relation=tsne_embeddings_relations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7BVz79k-qPU"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEo6b9-a-qPU"
   },
   "source": [
    "[1] Q. Yan, J. Fan, M. Li, G. Qu and Y. Xiao, \"A Survey on Knowledge Graph Embedding,\"¬†2022 7th IEEE International Conference on Data Science in Cyberspace (DSC), Guilin, China, 2022, pp. 576-583, doi: 10.1109/DSC55868.2022.00086.\n",
    "\n",
    "[2] Poliƒçar, Pavlin G., Martin Stra≈æar, and Bla≈æ Zupan. \"openTSNE: a modular Python library for t-SNE dimensionality reduction and embedding.\" Journal of Statistical Software 109 (2024): 1-30."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11c93da9a67b415694c66fdd5680bb4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a72b4391dd04d7a852737599835ff96": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_8b30fd5fa18c431c823c6f298f499af7",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\u001b[1;35mEvaluate recommendation\u001b[0m<span style=\"color: #800080; text-decoration-color: #800080\">  86%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ</span> <span style=\"color: #008000; text-decoration-color: #008000\">407/472 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">574 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[1;35mEvaluate recommendation\u001b[0m\u001b[35m  86%\u001b[0m \u001b[38;2;249;38;114m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m407/472 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m574 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "74323d8a3cd7470790c8ab50d9bdf09f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b30fd5fa18c431c823c6f298f499af7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7e7dd88fb89458cb99e23d728d5241a": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_74323d8a3cd7470790c8ab50d9bdf09f",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\u001b[1;35mTrain     0\u001b[0m<span style=\"color: #800080; text-decoration-color: #800080\">  90%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> <span style=\"color: #008000; text-decoration-color: #008000\">35/39 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:01</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">28 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[1;35mTrain     0\u001b[0m\u001b[35m  90%\u001b[0m \u001b[38;2;249;38;114m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;2;249;38;114m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35/39 \u001b[0m [ \u001b[33m0:00:01\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m28 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "ce6b7eeabc0a43bfa59002364580d207": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_11c93da9a67b415694c66fdd5680bb4d",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\u001b[1;35mEvaluate recommendation\u001b[0m<span style=\"color: #800080; text-decoration-color: #800080\">  98%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">‚ï∫</span> <span style=\"color: #008000; text-decoration-color: #008000\">463/472 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">579 it/s</span> ]\n</pre>\n",
         "text/plain": "\u001b[1;35mEvaluate recommendation\u001b[0m\u001b[35m  98%\u001b[0m \u001b[38;2;249;38;114m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;5;237m‚ï∫\u001b[0m \u001b[32m463/472 \u001b[0m [ \u001b[33m0:00:00\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m579 it/s\u001b[0m ]\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
